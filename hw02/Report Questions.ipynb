{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ee222f-e508-493f-8d80-dcebb19c76e7",
   "metadata": {},
   "source": [
    "# Report Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674ba1f-ff3f-4515-943c-d7c6dfbb1235",
   "metadata": {},
   "source": [
    "## 1 \n",
    "### Implement 2 models with approximately the same number of parameters\n",
    "### - (A) narrower and deeper (e.g. hidden_layers=6, hidden_dim=1024)\n",
    "### - (B) wider and shallower (e.g. hidden_layers=2, hidden_dim=1750)\n",
    "### Report training/validation accuracies for both models\n",
    "\n",
    "#### Experiment 1\n",
    "The following test will be used the same hyper-parameters such as:\n",
    "\n",
    "``` python\n",
    "# concat_nframes = 21\n",
    "# num_epoch = 30 \n",
    "# learning_rate = 1e-3\n",
    "# BatchNorm1d_DID \n",
    "# NO_Dropout Dropout由于会大幅降低训练速度，可能导致对比结果太低看不出区别，所以直接取消\n",
    "```\n",
    "\n",
    "|No|Param|Train Acc|Val Acc|Cost Time|\n",
    "|-----------|-----------|-------------|-------------|-------------|\n",
    "|1-深|hidden_layers=6, hidden_dim=1024|0.98181|<font color=\"red\">0.69268</font>|19 minutes 23 seconds local|\n",
    "|2-宽|hidden_layers=2, hidden_dim=1750|<font color=\"red\">0.98946</font>|0.68692|23 minutes 40 seconds local|\n",
    "\n",
    "#### Experiment 1 Report\n",
    "1-深在验证集表现上要好于2-宽\n",
    "而且由于2-宽在训练集上表现更好，但在验证集上表现更差，我怀疑可能2-宽会在epoch更大时，Train和Valid的反差更明显，所以后续将加大epoch进行实验。\n",
    "\n",
    "#### Experiment 2\n",
    "The following test will be used the same hyper-parameters such as:\n",
    "\n",
    "``` python\n",
    "# concat_nframes = 21\n",
    "# num_epoch = 100 训练时间改为100分钟，大约耗时1.5H \n",
    "# learning_rate = 1e-3\n",
    "# BatchNorm1d_DID \n",
    "# NO_Dropout Dropout由于会大幅降低训练速度，可能导致对比结果太低看不出区别，所以直接取消\n",
    "```\n",
    "\n",
    "|No|Param|Train Acc|Val Acc|Cost Time|\n",
    "|-----------|-----------|-------------|-------------|-------------|\n",
    "|1-深|hidden_layers=6, hidden_dim=1024|0.99493|<font color=\"red\">0.69810</font>|76 minutes 51 seconds colab T4|\n",
    "|2-宽|hidden_layers=2, hidden_dim=1750|<font color=\"red\">0.99684</font>|0.68719|61 minutes 20 seconds local|\n",
    "\n",
    "#### Experiment 2 Report\n",
    "第二次整体实验加大了epoch的数量，1-深的表现仍然更好。\n",
    "2-宽在加大了epoch后，验证集相比上一次，仅增长了0.001，是所有数据中增长最慢的。所以采取1-深的网络结果会更好。\n",
    "\n",
    "#### Conclusion\n",
    "经过两次实验，最终模型<font size=14>1-深</font>在本次实验中结果更好，今后也将使用1-深来进行后续实验。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38783283-d0e4-41b6-94b2-670ace62d8d0",
   "metadata": {},
   "source": [
    "## 2\n",
    "### Add dropout layers, and report training/validation accuracies with dropout rates equal to:\n",
    "### - (A) 0.25\n",
    "### - (B) 0.5\n",
    "### - (C) 0.75\n",
    "\n",
    "#### Experiment 1\n",
    "\n",
    "The following test will be used the same hyper-parameters such as:\n",
    "\n",
    "``` python\n",
    "# concat_nframes = 21\n",
    "# num_epoch = 30 \n",
    "# learning_rate = 1e-3\n",
    "# BatchNorm1d_DID\n",
    "# hidden_layers = 2\n",
    "# hidden_dim = 1750\n",
    "```\n",
    "\n",
    "|No|Param|Train Acc|Val Acc|Cost Time|\n",
    "|-----------|-----------|-------------|-------------|-------------|\n",
    "|1|dropout=0.25|0.81067|<font color=\"red\">0.74093<font>|49 minutes 54 seconds,concat_nframes: 21 ,<font color=\"red\">num_epoch: 60</font> ,learning_rate: 0.001|\n",
    "|2|dropout=0.5|0.73358|<font color=\"blue\">0.74728</font>/0.72807(60)|304 minutes 41 seconds,concat_nframes: 21 ,<font color=\"blue\">num_epoch: 600</font> ,learning_rate: 0.001|\n",
    "|3|dropout=0.75|0.69939|0.72817|47 minutes 15 seconds,concat_nframes: 21 ,num_epoch: 60 ,learning_rate: 0.001|\n",
    "\n",
    "#### Experiment 1 Report\n",
    "实验证明在1-深网络下，采取drop=0.25时，在epoch为60时效率明显比0.5和0.75高不少。但考虑到dropout本来就会让训练能力降低，所以需要将drop为0.25时，也同时训练600个epoch作为参考。\n",
    "    \n",
    "基于深/宽网络以及dropout数值和网上找到的实验结果存在明显差异（网上结果为：宽网络，dropout=0.5），我会在网上再搜索一遍资料，可能会重新测试\n",
    "\n",
    "网上搜查得到的一些结果：\n",
    "1. 深网络就是比宽网络的结果要好，且是指数级别的差距https://www.bilibili.com/video/BV1Wv411h7kN?p=33\n",
    "2. epoch不用太大，控制在250应该就能取得不错的成绩。且现在是2023年，我的显卡理应花更少的时间就能完成250次训练\n",
    "3. 大多数好的模型使用的dropout是0.35，原因未知，但也和0.25得到的结果相近，可以继续\n",
    "4. 关于`train_ratio`，其实可以调整的。在百万级别的数据面前，调到0.95更好，更具andrew ng的资料得来\n",
    "5. concat_nframes太多可能会得到反效果，因为每个phoneme最大不会超过240，11个frames的长度为(11*25=275),刚刚好，但如果太长，导致能融入两个phoneme（比如22个，现在我已经设到21，有些紧张了），那可能会导致识别出现误差。后续也会试着调小，个人觉得15~17是比较合适的。\n",
    "    \n",
    "## Experiment 2\n",
    "先保持`train_ratio`仍然=0.75进行dropout的测试，epoch调整到250次。在实验过后，将`train_ratio`调整到0.95以更好训练模型。\n",
    "    \n",
    "[146/600] Train Acc: 0.82772 Loss: 0.50341 | Val Acc: 0.75312 loss: 0.89568\n",
    "saving model with acc 0.75312\n",
    "    \n",
    "基于模型1，有把train_ratio从0.75改为0.95，在训练到146时，已经叨叨.0753,飞过strong base line\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816f7da-6be3-4ce0-8cc2-62c12182dddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
